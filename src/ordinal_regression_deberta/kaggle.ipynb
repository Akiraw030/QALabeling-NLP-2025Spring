{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684cd4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tqdm.auto import tqdm\n",
    "import html\n",
    "\n",
    "# ------------------- Configuration -------------------\n",
    "class CFG:\n",
    "    # Path to trained weights\n",
    "    model_dir = \"/kaggle/input/deberta-finetuned/pytorch/arch1/11\" \n",
    "    \n",
    "    # Tokenizer path\n",
    "    base_model = \"/kaggle/input/deberta-tokenizer/deberta-v3-base-tokenizer\" \n",
    "    \n",
    "    # Must match training setup\n",
    "    pooling_strategy = 'arch1_6groups' \n",
    "    \n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    seed = 42\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TARGET_COLS = [\n",
    "    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "    'answer_type_reason_explanation', 'answer_well_written'\n",
    "],\n",
    "\n",
    "# Reordered to align with 6-head grouping\n",
    "GROUP_ORDER_INDICES = [\n",
    "    3, 4, 5, 16, 17,          # G1\n",
    "    0, 1, 6, 7, 20,           # G2\n",
    "    2, 10,                    # G3\n",
    "    8, 9, 11, 12, 13, 14, 15, 18, 19, # G4\n",
    "    26, 27,                   # G5\n",
    "    21, 22, 23, 24, 25, 28, 29 # G6\n",
    "]\n",
    "SORTED_TARGET_COLS = [TARGET_COLS[i] for i in GROUP_ORDER_INDICES]\n",
    "\n",
    "class OptimizedRounder:\n",
    "    def __init__(self):\n",
    "        # Looser clip bounds for binary ordinal outputs that are already calibrated\n",
    "        self.coef_ = [0.025, 0.975]\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        # 1. Handle NaN\n",
    "        X = np.nan_to_num(X, nan=0.5)\n",
    "        X_p = np.copy(X)\n",
    "        low, high = coef[0], coef[1]\n",
    "        \n",
    "        # 2. Clip\n",
    "        X_p = np.clip(X_p, low, high)\n",
    "        \n",
    "        # 3. Avoid constant outputs (tiny perturbation)\n",
    "        if np.unique(X_p).size == 1:\n",
    "            eps = 1e-6\n",
    "            # Nudge based on original trend to avoid zero denominator in Spearman\n",
    "            if X_p[0] == low:\n",
    "                max_idx = np.argmax(X)\n",
    "                X_p[max_idx] += eps\n",
    "            elif X_p[0] == high:\n",
    "                min_idx = np.argmin(X)\n",
    "                X_p[min_idx] -= eps\n",
    "            \n",
    "        return X_p\n",
    "\n",
    "# ------------------- Core Module: Binary Target Encoder -------------------\n",
    "class BinaryTargetEncoder:\n",
    "    \"\"\"Define output dimensions and restore scores.\"\"\"\n",
    "    def __init__(self, target_cols=SORTED_TARGET_COLS):\n",
    "        self.target_cols = target_cols\n",
    "        self.unique_values = {} \n",
    "        self.thresholds = {}    \n",
    "        self.output_slices = {} \n",
    "        self.total_output_dim = 0\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Critical: read train.csv to determine bit counts per column so the model head matches.\n",
    "        \"\"\"\n",
    "        print(f\"Fitting Binary Encoder on {len(df)} samples...\")\n",
    "        current_idx = 0\n",
    "        for col in self.target_cols:\n",
    "            uniques = sorted(df[col].unique())\n",
    "            self.unique_values[col] = uniques\n",
    "            \n",
    "            if len(uniques) > 1:\n",
    "                thresh = uniques[:-1]\n",
    "            else:\n",
    "                thresh = [uniques[0]] \n",
    "                \n",
    "            self.thresholds[col] = thresh\n",
    "            n_dims = len(thresh)\n",
    "            self.output_slices[col] = slice(current_idx, current_idx + n_dims)\n",
    "            current_idx += n_dims\n",
    "            \n",
    "        self.total_output_dim = current_idx\n",
    "        print(f\"Total Binary Output Dimension: {self.total_output_dim}\")\n",
    "\n",
    "    def inverse_transform(self, binary_preds):\n",
    "        \"\"\"\n",
    "        Input: (Batch, Total_Binary_Dim) - probabilities\n",
    "        Output: (Batch, 30) - restored scores\n",
    "        \"\"\"\n",
    "        batch_size = binary_preds.shape[0]\n",
    "        # Output order follows self.target_cols (SORTED_TARGET_COLS)\n",
    "        output = np.zeros((batch_size, len(self.target_cols)), dtype=np.float32)\n",
    "        \n",
    "        for i, col in enumerate(self.target_cols):\n",
    "            slc = self.output_slices[col]\n",
    "            col_preds = binary_preds[:, slc]\n",
    "            # Restore via mean (expected value approximation)\n",
    "            output[:, i] = col_preds.mean(axis=1)\n",
    "            \n",
    "        return output\n",
    "\n",
    "# ------------------- Data Processing -------------------\n",
    "def modern_preprocess(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "class QuestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.questions = [\n",
    "            modern_preprocess(t) + \" \" + modern_preprocess(b) \n",
    "            for t, b in zip(df['question_title'].values, df['question_body'].values)\n",
    "        ]\n",
    "        self.answers = [modern_preprocess(a) for a in df['answer'].values]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            question,\n",
    "            answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if 'token_type_ids' in inputs:\n",
    "            item['token_type_ids'] = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# ------------------- Model Definition (dynamic output dims) -------------------\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, model_name, target_encoder, pooling_strategy='arch1_6groups', dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        # Initialize from config (offline-friendly for Kaggle)\n",
    "        self.backbone = AutoModel.from_config(self.config)\n",
    "        hidden_size = self.config.hidden_size\n",
    "        \n",
    "        # 1. Compute output dims per group\n",
    "        all_cols = target_encoder.target_cols \n",
    "        \n",
    "        # Slice groups according to SORTED_TARGET_COLS order\n",
    "        g1_cols = all_cols[0:5]\n",
    "        g2_cols = all_cols[5:10]\n",
    "        g3_cols = all_cols[10:12]\n",
    "        g4_cols = all_cols[12:21]\n",
    "        g5_cols = all_cols[21:23]\n",
    "        g6_cols = all_cols[23:30]\n",
    "        \n",
    "        self.group_dims = {}\n",
    "        for g_name, cols in zip(['g1','g2','g3','g4','g5','g6'], [g1_cols, g2_cols, g3_cols, g4_cols, g5_cols, g6_cols]):\n",
    "            dim = 0\n",
    "            for c in cols:\n",
    "                slc = target_encoder.output_slices[c]\n",
    "                dim += (slc.stop - slc.start)\n",
    "            self.group_dims[g_name] = dim\n",
    "            \n",
    "        # 2. Define heads\n",
    "        if self.pooling_strategy == 'arch1_6groups':\n",
    "            self.head_g1 = self._make_head(hidden_size * 3, self.group_dims['g1'], dropout_rate)\n",
    "            self.head_g2 = self._make_head(hidden_size * 3, self.group_dims['g2'], dropout_rate)\n",
    "            self.head_g3 = self._make_head(hidden_size * 3, self.group_dims['g3'], dropout_rate)\n",
    "            self.head_g4 = self._make_head(hidden_size * 3, self.group_dims['g4'], dropout_rate)\n",
    "            self.head_g5 = self._make_head(hidden_size * 3, self.group_dims['g5'], dropout_rate)\n",
    "            self.head_g6 = self._make_head(hidden_size * 3, self.group_dims['g6'], dropout_rate)\n",
    "\n",
    "    def _make_head(self, input_dim, output_dim, dropout_rate):\n",
    "        head = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.config.hidden_size, output_dim)\n",
    "        )\n",
    "        return head\n",
    "\n",
    "    def _masked_mean_pooling(self, hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def _get_pooling_features(self, last_hidden_state, attention_mask, token_type_ids):\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        global_avg = self._masked_mean_pooling(last_hidden_state, attention_mask)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            q_avg = global_avg; a_avg = global_avg\n",
    "        else:\n",
    "            q_mask = attention_mask * (1 - token_type_ids)\n",
    "            q_avg = self._masked_mean_pooling(last_hidden_state, q_mask)\n",
    "            a_mask = attention_mask * token_type_ids\n",
    "            a_avg = self._masked_mean_pooling(last_hidden_state, a_mask)\n",
    "            \n",
    "        return cls_token, global_avg, q_avg, a_avg\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling_strategy == 'arch1_6groups':\n",
    "            cls, glob, q, a = self._get_pooling_features(last_hidden_state, attention_mask, token_type_ids)\n",
    "            \n",
    "            feat_pure_q = torch.cat([cls, glob, q], dim=1)\n",
    "            feat_pure_a = torch.cat([cls, glob, a], dim=1)\n",
    "            \n",
    "            out_g1 = self.head_g1(feat_pure_q)\n",
    "            out_g2 = self.head_g2(feat_pure_q)\n",
    "            out_g3 = self.head_g3(feat_pure_q)\n",
    "            out_g4 = self.head_g4(feat_pure_q)\n",
    "            out_g5 = self.head_g5(feat_pure_a)\n",
    "            out_g6 = self.head_g6(feat_pure_a)\n",
    "            \n",
    "            # Directly concatenate to form the long binary vector\n",
    "            output = torch.cat([out_g1, out_g2, out_g3, out_g4, out_g5, out_g6], dim=1)\n",
    "            return output\n",
    "        \n",
    "        return None\n",
    "\n",
    "# ------------------- Inference Logic -------------------\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch.get('token_type_ids')\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            y_preds = model(input_ids, attention_mask, token_type_ids)\n",
    "            # Binary logits -> Sigmoid -> Probability\n",
    "            preds.append(y_preds.sigmoid().cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "if __name__ == '__main__':\n",
    "    # 1. Load data (train initializes encoder; test for inference)\n",
    "    TRAIN_PATH = '/kaggle/input/google-quest-challenge/train.csv'\n",
    "    TEST_PATH = '/kaggle/input/google-quest-challenge/test.csv'\n",
    "    \n",
    "    # Local fallback\n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        TRAIN_PATH = 'train.csv'\n",
    "        TEST_PATH = 'test.csv'\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    print(f\"Train Shape: {train_df.shape}, Test Shape: {test_df.shape}\")\n",
    "    \n",
    "    # 2. Initialize and fit encoder (sets model output shape)\n",
    "    target_encoder = BinaryTargetEncoder(target_cols=SORTED_TARGET_COLS)\n",
    "    target_encoder.fit(train_df)\n",
    "    \n",
    "    # 3. Prepare tokenizer & dataloader\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.base_model)\n",
    "    test_dataset = QuestDataset(test_df, tokenizer, max_len=CFG.max_len)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 4. Find model weights\n",
    "    weight_paths = []\n",
    "    for fold in range(5):\n",
    "        path = os.path.join(CFG.model_dir, f\"deberta_v3_fold{fold}_best.pth\")\n",
    "        if os.path.exists(path): weight_paths.append(path)\n",
    "    if os.path.exists(os.path.join(CFG.model_dir, \"deberta_v3_single_run_best.pth\")):\n",
    "        weight_paths.append(os.path.join(CFG.model_dir, \"deberta_v3_single_run_best.pth\"))\n",
    "        \n",
    "    if not weight_paths:\n",
    "        print(\"No weights found!\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"Found {len(weight_paths)} models.\")\n",
    "\n",
    "    # 5. Inference and reconstruction\n",
    "    final_preds_accum = []\n",
    "    \n",
    "    for weight_path in weight_paths:\n",
    "        print(f\"Predicting with {os.path.basename(weight_path)}...\")\n",
    "        \n",
    "        # Build model with encoder-defined dimensions\n",
    "        model = QuestModel(\n",
    "            CFG.base_model, \n",
    "            target_encoder=target_encoder,\n",
    "            pooling_strategy=CFG.pooling_strategy\n",
    "        )\n",
    "        \n",
    "        state_dict = torch.load(weight_path, map_location=CFG.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(CFG.device)\n",
    "        \n",
    "        # Binary probability predictions (Batch, Total_Dim)\n",
    "        binary_preds = inference_fn(test_loader, model, CFG.device)\n",
    "        \n",
    "        # Restore continuous scores (Batch, 30) in SORTED_TARGET_COLS order\n",
    "        decoded_preds = target_encoder.inverse_transform(binary_preds)\n",
    "        final_preds_accum.append(decoded_preds)\n",
    "        \n",
    "        del model, state_dict\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    # 6. Average and write submission\n",
    "    if len(final_preds_accum) > 0:\n",
    "        # Raw restored scores before rounding\n",
    "        avg_preds = np.mean(final_preds_accum, axis=0)\n",
    "        \n",
    "        # Apply OptimizedRounder\n",
    "        print(\"Applying OptimizedRounder...\")\n",
    "        final_preds = np.zeros_like(avg_preds)\n",
    "        opt = OptimizedRounder()\n",
    "        \n",
    "        # Clip each column\n",
    "        for i in range(len(TARGET_COLS)):\n",
    "            final_preds[:, i] = opt.predict(avg_preds[:, i], opt.coef_)\n",
    "        \n",
    "        # Build submission DataFrame\n",
    "        submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n",
    "        \n",
    "        # Map sorted columns back to original TARGET_COLS order\n",
    "        pred_df = pd.DataFrame(final_preds, columns=SORTED_TARGET_COLS)\n",
    "        \n",
    "        for col in TARGET_COLS:\n",
    "            submission[col] = pred_df[col]\n",
    "            \n",
    "        submission.to_csv('submission.csv', index=False)\n",
    "        print(\"submission.csv saved successfully! (With OptimizedRounder)\")\n",
    "    else:\n",
    "        print(\"Error during inference.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
