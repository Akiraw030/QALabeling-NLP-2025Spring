{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Kaggle Inference Notebook\n",
    "# Model: DeBERTa v3 (Arch1 - 6 Grouped Heads Strategy)\n",
    "# ====================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tqdm.auto import tqdm\n",
    "import html\n",
    "\n",
    "# ------------------- 配置 (Configuration) -------------------\n",
    "class CFG:\n",
    "    # 您的訓練權重路徑 (請指向存放 arch1_6groups 權重的資料夾)\n",
    "    model_dir = \"/kaggle/input/deberta-finetuned/pytorch/arch1/8\" \n",
    "    \n",
    "    # 您的 Tokenizer 資料夾路徑\n",
    "    base_model = \"/kaggle/input/deberta-tokenizer/deberta-v3-base-tokenizer\" \n",
    "    \n",
    "    # 必須與訓練時一致！\n",
    "    pooling_strategy = 'arch1_6groups' \n",
    "    \n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TARGET_COLS = [\n",
    "    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "    'answer_type_reason_explanation', 'answer_well_written'\n",
    "]\n",
    "\n",
    "# ------------------- 資料處理 -------------------\n",
    "def modern_preprocess(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "class QuestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.questions = [\n",
    "            modern_preprocess(t) + \" \" + modern_preprocess(b) \n",
    "            for t, b in zip(df['question_title'].values, df['question_body'].values)\n",
    "        ]\n",
    "        self.answers = [modern_preprocess(a) for a in df['answer'].values]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            question,\n",
    "            answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if 'token_type_ids' in inputs:\n",
    "            item['token_type_ids'] = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# ------------------- 模型定義 (6-Head 架構) -------------------\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, model_name, num_targets, pooling_strategy='arch1', dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        if pooling_strategy == 'arch2':\n",
    "            self.config.update({'output_hidden_states': True})\n",
    "            \n",
    "        # 使用 from_config 初始化結構 (不載入預訓練權重，因為我們會覆蓋它)\n",
    "        self.backbone = AutoModel.from_config(self.config)\n",
    "        hidden_size = self.config.hidden_size\n",
    "        \n",
    "        # -----------------------------------------------------------------------\n",
    "        # 定義 6-Head 分群索引 (QA Splitted Strategy)\n",
    "        # -----------------------------------------------------------------------\n",
    "        # Question Groups (G1-G4)\n",
    "        self.idx_g1 = [3, 4, 5, 16, 17]          # Fact/Instructions\n",
    "        self.idx_g2 = [0, 1, 6, 7, 20]           # Quality/Intent\n",
    "        self.idx_g3 = [2, 10]                    # Conversational\n",
    "        self.idx_g4 = [8, 9, 11, 12, 13, 14, 15, 18, 19] # Type/Class\n",
    "        \n",
    "        # Answer Groups (G5-G6)\n",
    "        self.idx_g5 = [26, 27]                   # Instructions\n",
    "        self.idx_g6 = [21, 22, 23, 24, 25, 28, 29] # Quality/Helpful\n",
    "        \n",
    "        # -----------------------------------------------------------------------\n",
    "        # 定義 Heads\n",
    "        # -----------------------------------------------------------------------\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            self.fc = nn.Linear(hidden_size, num_targets)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1':\n",
    "            # 舊版 2-Head 邏輯\n",
    "            self.q_head = self._make_head(hidden_size * 3, 21, dropout_rate)\n",
    "            self.a_head = self._make_head(hidden_size * 3, 9, dropout_rate)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1_6groups':\n",
    "            # --- 6-Head 策略 (QA Splitted) ---\n",
    "            # 所有的 Head 輸入都是 3 * hidden (因為只用 Q 特徵或 A 特徵)\n",
    "            # Question Heads\n",
    "            self.head_g1 = self._make_head(hidden_size * 3, len(self.idx_g1), dropout_rate)\n",
    "            self.head_g2 = self._make_head(hidden_size * 3, len(self.idx_g2), dropout_rate)\n",
    "            self.head_g3 = self._make_head(hidden_size * 3, len(self.idx_g3), dropout_rate)\n",
    "            self.head_g4 = self._make_head(hidden_size * 3, len(self.idx_g4), dropout_rate)\n",
    "            \n",
    "            # Answer Heads\n",
    "            self.head_g5 = self._make_head(hidden_size * 3, len(self.idx_g5), dropout_rate)\n",
    "            self.head_g6 = self._make_head(hidden_size * 3, len(self.idx_g6), dropout_rate)\n",
    "\n",
    "        elif self.pooling_strategy == 'arch2':\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_size * 4, num_targets)\n",
    "            )\n",
    "\n",
    "    def _make_head(self, input_dim, output_dim, dropout_rate):\n",
    "        head = nn.Sequential(\n",
    "            nn.Linear(input_dim, self.config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.config.hidden_size, output_dim)\n",
    "        )\n",
    "        return head\n",
    "\n",
    "    def _masked_mean_pooling(self, hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def _get_pooling_features(self, last_hidden_state, attention_mask, token_type_ids):\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        global_avg = self._masked_mean_pooling(last_hidden_state, attention_mask)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            q_avg = global_avg\n",
    "            a_avg = global_avg\n",
    "        else:\n",
    "            q_mask = attention_mask * (1 - token_type_ids)\n",
    "            q_avg = self._masked_mean_pooling(last_hidden_state, q_mask)\n",
    "            a_mask = attention_mask * token_type_ids\n",
    "            a_avg = self._masked_mean_pooling(last_hidden_state, a_mask)\n",
    "            \n",
    "        return cls_token, global_avg, q_avg, a_avg\n",
    "\n",
    "    def _pool_arch2(self, all_hidden_states):\n",
    "        last_4_layers = all_hidden_states[-4:]\n",
    "        cls_embeddings = [layer[:, 0, :] for layer in last_4_layers]\n",
    "        return torch.cat(cls_embeddings, dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling_strategy == 'mean':\n",
    "            feature = self._masked_mean_pooling(last_hidden_state, attention_mask)\n",
    "            output = self.fc(feature)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1':\n",
    "            cls, glob, q, a = self._get_pooling_features(last_hidden_state, attention_mask, token_type_ids)\n",
    "            q_feat = torch.cat([cls, glob, q], dim=1)\n",
    "            a_feat = torch.cat([cls, glob, a], dim=1)\n",
    "            output = torch.cat([self.q_head(q_feat), self.a_head(a_feat)], dim=1)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1_6groups':\n",
    "            cls, glob, q, a = self._get_pooling_features(last_hidden_state, attention_mask, token_type_ids)\n",
    "            \n",
    "            # 1. 準備特徵 (3x Hidden)\n",
    "            # Question 特徵：[CLS, Global, Q_Avg]\n",
    "            feat_pure_q = torch.cat([cls, glob, q], dim=1)\n",
    "            # Answer 特徵：[CLS, Global, A_Avg]\n",
    "            feat_pure_a = torch.cat([cls, glob, a], dim=1)\n",
    "            \n",
    "            # 2. 通過各個 Head\n",
    "            # Q Groups 使用 Q 特徵\n",
    "            out_g1 = self.head_g1(feat_pure_q)\n",
    "            out_g2 = self.head_g2(feat_pure_q)\n",
    "            out_g3 = self.head_g3(feat_pure_q)\n",
    "            out_g4 = self.head_g4(feat_pure_q)\n",
    "            \n",
    "            # A Groups 使用 A 特徵\n",
    "            out_g5 = self.head_g5(feat_pure_a)\n",
    "            out_g6 = self.head_g6(feat_pure_a)\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            # 確保 dtype 一致性 (for mixed precision)\n",
    "            output = torch.zeros(batch_size, 30, dtype=out_g1.dtype, device=input_ids.device)\n",
    "            \n",
    "            output[:, self.idx_g1] = out_g1\n",
    "            output[:, self.idx_g2] = out_g2\n",
    "            output[:, self.idx_g3] = out_g3\n",
    "            output[:, self.idx_g4] = out_g4\n",
    "            output[:, self.idx_g5] = out_g5\n",
    "            output[:, self.idx_g6] = out_g6\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch2':\n",
    "            feature = self._pool_arch2(outputs.hidden_states)\n",
    "            output = self.fc(feature)\n",
    "            \n",
    "        return output\n",
    "\n",
    "# ------------------- 優化器 (Robust OptimizedRounder) -------------------\n",
    "class OptimizedRounder:\n",
    "    def __init__(self):\n",
    "        self.coef_ = [0.015, 0.985]\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X = np.nan_to_num(X, nan=0.5)\n",
    "        X_p = np.copy(X)\n",
    "        low, high = coef[0], coef[1]\n",
    "        X_p = np.clip(X_p, low, high)\n",
    "        \n",
    "        if np.unique(X_p).size == 1:\n",
    "            eps = 1e-3\n",
    "            max_idx = np.argmax(X)\n",
    "            X_p[max_idx] += eps\n",
    "            \n",
    "        return X_p\n",
    "\n",
    "# ------------------- 推論邏輯 -------------------\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            token_type_ids = batch.get('token_type_ids')\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            y_preds = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds.append(y_preds.sigmoid().cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# ------------------- 主程式 -------------------\n",
    "if __name__ == '__main__':\n",
    "    TEST_PATH = 'test.csv'\n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        TEST_PATH = '/kaggle/input/google-quest-challenge/test.csv'\n",
    "    \n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print(f\"Test Data Shape: {test.shape}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.base_model)\n",
    "    \n",
    "    test_dataset = QuestDataset(test, tokenizer, max_len=CFG.max_len)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    fold_preds = []\n",
    "    \n",
    "    for fold in range(CFG.n_fold):\n",
    "        weight_path = os.path.join(CFG.model_dir, f\"deberta_v3_fold{fold}_best.pth\")\n",
    "        \n",
    "        if not os.path.exists(weight_path):\n",
    "            print(f\"Warning: Weights for fold {fold} not found at {weight_path}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading Fold {fold} Model...\")\n",
    "        \n",
    "        model = QuestModel(\n",
    "            CFG.base_model, \n",
    "            num_targets=len(TARGET_COLS), \n",
    "            pooling_strategy=CFG.pooling_strategy\n",
    "        )\n",
    "        \n",
    "        state_dict = torch.load(weight_path, map_location=CFG.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(CFG.device)\n",
    "        \n",
    "        preds = inference_fn(test_loader, model, CFG.device)\n",
    "        fold_preds.append(preds)\n",
    "        \n",
    "        del model, state_dict\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    if len(fold_preds) > 0:\n",
    "        avg_preds = np.mean(fold_preds, axis=0)\n",
    "        \n",
    "        print(\"Applying OptimizedRounder...\")\n",
    "        final_preds = np.zeros_like(avg_preds)\n",
    "        opt = OptimizedRounder()\n",
    "        \n",
    "        for i in range(len(TARGET_COLS)):\n",
    "            final_preds[:, i] = opt.predict(avg_preds[:, i], opt.coef_)\n",
    "            \n",
    "        submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n",
    "        submission[TARGET_COLS] = final_preds\n",
    "        submission.to_csv('submission.csv', index=False)\n",
    "        print(\"submission.csv saved successfully!\")\n",
    "    else:\n",
    "        print(\"Error: No predictions generated.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 828965,
     "sourceId": 7968,
     "sourceType": "competition"
    },
    {
     "datasetId": 8889245,
     "sourceId": 13946897,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 521159,
     "modelInstanceId": 506367,
     "sourceId": 668898,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
