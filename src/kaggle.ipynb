{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7968,"databundleVersionId":828965,"sourceType":"competition"},{"sourceId":13946897,"sourceType":"datasetVersion","datasetId":8889245},{"sourceId":668898,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":506367,"modelId":521159}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Kaggle Inference Notebook (With Optimization)\n# ====================================================\n\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom tqdm.auto import tqdm\nimport html\n\n# ------------------- 配置 (Configuration) -------------------\nclass CFG:\n    # 您的訓練權重路徑\n    model_dir = \"/kaggle/input/deberta-finetuned/pytorch/arch1/3\" \n    \n    # 您的 Tokenizer 資料夾路徑\n    base_model = \"/kaggle/input/deberta-tokenizer/deberta-v3-base-tokenizer\" \n    \n    pooling_strategy = 'arch1'\n    max_len = 512\n    batch_size = 16\n    num_workers = 2\n    seed = 42\n    n_fold = 5\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nTARGET_COLS = [\n    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n    'answer_type_reason_explanation', 'answer_well_written'\n]\n\n# ------------------- 資料處理 -------------------\ndef modern_preprocess(text):\n    if pd.isna(text): return \"\"\n    text = str(text)\n    text = html.unescape(text)\n    text = \" \".join(text.split())\n    return text\n\nclass QuestDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=512):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n        self.questions = [\n            modern_preprocess(t) + \" \" + modern_preprocess(b) \n            for t, b in zip(df['question_title'].values, df['question_body'].values)\n        ]\n        self.answers = [modern_preprocess(a) for a in df['answer'].values]\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        answer = self.answers[idx]\n        \n        inputs = self.tokenizer(\n            question,\n            answer,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors=None\n        )\n        \n        item = {\n            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        }\n        \n        if 'token_type_ids' in inputs:\n            item['token_type_ids'] = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n            \n        return item\n\n# ------------------- 模型定義 -------------------\nclass QuestModel(nn.Module):\n    def __init__(self, model_name, num_targets, pooling_strategy='arch1'):\n        super().__init__()\n        self.pooling_strategy = pooling_strategy\n        \n        self.config = AutoConfig.from_pretrained(model_name)\n        if pooling_strategy == 'arch2':\n            self.config.update({'output_hidden_states': True})\n            \n        # 使用 from_config 初始化結構\n        self.backbone = AutoModel.from_config(self.config)\n        \n        hidden_size = self.config.hidden_size\n        \n        if self.pooling_strategy == 'mean':\n            self.fc = nn.Linear(hidden_size, num_targets)\n            \n        elif self.pooling_strategy == 'arch1':\n            self.intermediate_layer = nn.Sequential(\n                nn.Linear(hidden_size * 5, hidden_size),\n                nn.Tanh(),\n                nn.Dropout(0.1)\n            )\n            self.fc = nn.Linear(hidden_size, num_targets)\n            \n        elif self.pooling_strategy == 'arch2':\n            self.fc = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Linear(hidden_size * 4, num_targets)\n            )\n\n    def _masked_mean_pooling(self, hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n\n    def _pool_arch1(self, last_hidden_state, attention_mask, token_type_ids):\n        batch_size = last_hidden_state.size(0)\n        cls_token = last_hidden_state[:, 0, :]\n        last_token_indices = attention_mask.sum(dim=1) - 1\n        last_token = last_hidden_state[torch.arange(batch_size), last_token_indices, :]\n        global_avg = self._masked_mean_pooling(last_hidden_state, attention_mask)\n        \n        if token_type_ids is None:\n            q_avg = global_avg\n            a_avg = global_avg\n        else:\n            q_mask = attention_mask * (1 - token_type_ids)\n            q_avg = self._masked_mean_pooling(last_hidden_state, q_mask)\n            a_mask = attention_mask * token_type_ids\n            a_avg = self._masked_mean_pooling(last_hidden_state, a_mask)\n            \n        return torch.cat([cls_token, last_token, global_avg, q_avg, a_avg], dim=1)\n\n    def _pool_arch2(self, all_hidden_states):\n        last_4_layers = all_hidden_states[-4:]\n        cls_embeddings = [layer[:, 0, :] for layer in last_4_layers]\n        return torch.cat(cls_embeddings, dim=1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        \n        if self.pooling_strategy == 'mean':\n            feature = self._masked_mean_pooling(outputs.last_hidden_state, attention_mask)\n            output = self.fc(feature)\n        elif self.pooling_strategy == 'arch1':\n            feature = self._pool_arch1(outputs.last_hidden_state, attention_mask, token_type_ids)\n            output = self.fc(self.intermediate_layer(feature))\n        elif self.pooling_strategy == 'arch2':\n            feature = self._pool_arch2(outputs.hidden_states)\n            output = self.fc(feature)\n            \n        return output\n\n# ------------------- 優化器 (OptimizedRounder) -------------------\nclass OptimizedRounder:\n    def __init__(self):\n        self.coef_ = [0.025, 0.975]\n\n    def predict(self, X, coef):\n        # 1. 處理 NaN: 如果有空值，先補成 0.5 (避免程式崩潰)\n        X = np.nan_to_num(X, nan=0.5)\n        \n        X_p = np.copy(X)\n        low, high = coef[0], coef[1]\n        \n        # 2. 執行截斷\n        X_p = np.clip(X_p, low, high)\n        \n        # 3. 檢查是否變成常數 (所有數值都一樣)\n        if np.unique(X_p).size == 1:\n            # 為了讓 Spearman 能計算，我們必須打破「所有數值都一樣」的狀態\n            # 我們製造一個極小的擾動 (epsilon)\n            eps = 1e-3\n            \n            # 策略：找出原始預測中「最大」的那個值，讓它在截斷後稍微大一點點\n            # 這能保證我們沒有破壞原本的排序邏輯 (原本大的，現在還是稍微大一點)\n            max_idx = np.argmax(X)\n            \n            # 強制加上擾動\n            X_p[max_idx] += eps\n            \n        return X_p\n\n# ------------------- 推論迴圈 -------------------\ndef inference_fn(test_loader, model, device):\n    model.eval()\n    preds = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Predicting\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            token_type_ids = batch.get('token_type_ids')\n            if token_type_ids is not None:\n                token_type_ids = token_type_ids.to(device)\n            \n            y_preds = model(input_ids, attention_mask, token_type_ids)\n            preds.append(y_preds.sigmoid().cpu().numpy())\n            \n    return np.concatenate(preds)\n\n# ------------------- 主程式 -------------------\nif __name__ == '__main__':\n    TEST_PATH = 'test.csv'\n    if not os.path.exists(TEST_PATH):\n        TEST_PATH = '/kaggle/input/google-quest-challenge/test.csv'\n    \n    test = pd.read_csv(TEST_PATH)\n    print(f\"Test Data Shape: {test.shape}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(CFG.base_model)\n    \n    test_dataset = QuestDataset(test, tokenizer, max_len=CFG.max_len)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=CFG.batch_size, \n        shuffle=False, \n        num_workers=CFG.num_workers,\n        pin_memory=True\n    )\n    \n    fold_preds = []\n    \n    for fold in range(CFG.n_fold):\n        weight_path = os.path.join(CFG.model_dir, f\"deberta_v3_fold{fold}_best.pth\")\n        \n        if not os.path.exists(weight_path):\n            print(f\"Warning: Weights for fold {fold} not found at {weight_path}. Skipping.\")\n            continue\n            \n        print(f\"Loading Fold {fold} Model...\")\n        \n        model = QuestModel(\n            CFG.base_model, \n            num_targets=len(TARGET_COLS), \n            pooling_strategy=CFG.pooling_strategy\n        )\n        \n        state_dict = torch.load(weight_path, map_location=CFG.device)\n        model.load_state_dict(state_dict)\n        model.to(CFG.device)\n        \n        preds = inference_fn(test_loader, model, CFG.device)\n        fold_preds.append(preds)\n        \n        del model, state_dict\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    if len(fold_preds) > 0:\n        # 1. 取得平均預測 (未優化)\n        avg_preds = np.mean(fold_preds, axis=0)\n        \n        # 2. 應用 OptimizedRounder (優化)\n        print(\"Applying OptimizedRounder...\")\n        final_preds = np.zeros_like(avg_preds)\n        opt = OptimizedRounder()\n        \n        # 針對每一行 (樣本) 的每一個欄位 (目標) 進行截斷\n        # 因為 OptimizedRounder 是針對 columns 操作的\n        for i in range(len(TARGET_COLS)):\n            final_preds[:, i] = opt.predict(avg_preds[:, i], opt.coef_)\n            \n        submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n        submission[TARGET_COLS] = final_preds\n        submission.to_csv('submission.csv', index=False)\n        print(\"submission.csv saved successfully! (With Optimization)\")\n    else:\n        print(\"Error: No predictions generated.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}