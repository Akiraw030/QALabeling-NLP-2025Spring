{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Kaggle Inference Notebook (Llama 3.2 1B + 4 Post-Processing Strategies)\n",
    "# ====================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import rankdata\n",
    "import html\n",
    "\n",
    "# ------------------- 配置 (Configuration) -------------------\n",
    "class CFG:\n",
    "    # 您的訓練權重路徑 (請放置 Llama 3.2 1B 微調後的模型權重)\n",
    "    model_dir = \"/kaggle/input/llama\"  # TODO: 修改為您在 Kaggle Input 的資料集路徑\n",
    "\n",
    "    # Tokenizer / Base Model (需與訓練時相同)\n",
    "    base_model = \"/kaggle/input/llama-3-2/transformers/1b/1\"\n",
    "\n",
    "    pooling_strategy = 'arch1_6groups' \n",
    "\n",
    "    # 【關鍵開關】選擇後處理方式\n",
    "    # Options: 'raw', 'optimized', 'voters', 'distribution'\n",
    "    post_processing = 'voters' \n",
    "\n",
    "    max_len = 512\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trust_remote_code = False\n",
    "    freeze_backbone = True\n",
    "\n",
    "TARGET_COLS = [\n",
    "    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "    'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "    'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "    'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "    'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "    'answer_type_reason_explanation', 'answer_well_written'\n",
    "]\n",
    "\n",
    "# ------------------- 資料處理 -------------------\n",
    "def modern_preprocess(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "class QuestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.questions = [\n",
    "            modern_preprocess(t) + \" \" + modern_preprocess(b) \n",
    "            for t, b in zip(df['question_title'].values, df['question_body'].values)\n",
    "        ]\n",
    "        self.answers = [modern_preprocess(a) for a in df['answer'].values]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            question,\n",
    "            answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        # Llama tokenizer 可能不提供 token_type_ids\n",
    "        if 'token_type_ids' in inputs:\n",
    "            item['token_type_ids'] = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# ------------------- 模型定義 (Single Regression) -------------------\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_targets,\n",
    "        pooling_strategy='arch1',\n",
    "        dropout_rate=0.1,\n",
    "        freeze_backbone=True,\n",
    "        trust_remote_code=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.config = AutoConfig.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "        if pooling_strategy == 'arch2':\n",
    "            self.config.output_hidden_states = True\n",
    "            \n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=self.config,\n",
    "            trust_remote_code=trust_remote_code\n",
    "        )\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.config.hidden_size\n",
    "        \n",
    "        self.idx_g1 = [3, 4, 5, 16, 17]          \n",
    "        self.idx_g2 = [0, 1, 6, 7, 20]           \n",
    "        self.idx_g3 = [2, 10]                    \n",
    "        self.idx_g4 = [8, 9, 11, 12, 13, 14, 15, 18, 19] \n",
    "        self.idx_g5 = [26, 27]                   \n",
    "        self.idx_g6 = [21, 22, 23, 24, 25, 28, 29] \n",
    "        \n",
    "        if self.pooling_strategy == 'mean':\n",
    "            self.fc = nn.Linear(hidden_size, num_targets)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1':\n",
    "            self.q_head = self._make_head(hidden_size * 3, 21, dropout_rate)\n",
    "            self.a_head = self._make_head(hidden_size * 3, 9, dropout_rate)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1_6groups':\n",
    "            self.head_g1 = self._make_head(hidden_size * 3, len(self.idx_g1), dropout_rate)\n",
    "            self.head_g2 = self._make_head(hidden_size * 3, len(self.idx_g2), dropout_rate)\n",
    "            self.head_g3 = self._make_head(hidden_size * 3, len(self.idx_g3), dropout_rate)\n",
    "            self.head_g4 = self._make_head(hidden_size * 3, len(self.idx_g4), dropout_rate)\n",
    "            self.head_g5 = self._make_head(hidden_size * 3, len(self.idx_g5), dropout_rate)\n",
    "            self.head_g6 = self._make_head(hidden_size * 3, len(self.idx_g6), dropout_rate)\n",
    "        \n",
    "        elif self.pooling_strategy == 'arch2':\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(hidden_size * 4, num_targets)\n",
    "            )\n",
    "        \n",
    "    def _make_head(self, input_dim, output_dim, dropout_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, self.config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.config.hidden_size, output_dim)\n",
    "        )\n",
    "        \n",
    "    def _masked_mean_pooling(self, hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "        \n",
    "    def _last_token_pool(self, last_hidden_state, attention_mask):\n",
    "        # 支援 left-padding 與 right-padding\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_state[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_state.shape[0]\n",
    "            return last_hidden_state[torch.arange(batch_size, device=last_hidden_state.device), sequence_lengths]\n",
    "\n",
    "    def _get_pooling_features(self, last_hidden_state, attention_mask, token_type_ids):\n",
    "        # Llama uses mean pooling for better representation\n",
    "        global_avg = self._masked_mean_pooling(last_hidden_state, attention_mask)\n",
    "        last_token = self._last_token_pool(last_hidden_state, attention_mask)\n",
    "        \n",
    "        # Llama 無 token_type_ids；採用 shared feature\n",
    "        if token_type_ids is None:\n",
    "            q_repr = global_avg\n",
    "            a_repr = global_avg\n",
    "        else:\n",
    "            # Fallback for models with token_type_ids\n",
    "            q_mask = attention_mask * (1 - token_type_ids)\n",
    "            q_repr = self._masked_mean_pooling(last_hidden_state, q_mask)\n",
    "            a_mask = attention_mask * token_type_ids\n",
    "            a_repr = self._masked_mean_pooling(last_hidden_state, a_mask)\n",
    "        \n",
    "        return global_avg, last_token, q_repr, a_repr\n",
    "        \n",
    "    def _pool_arch2(self, all_hidden_states):\n",
    "        last_4_layers = all_hidden_states[-4:]\n",
    "        cls_embeddings = [layer[:, 0, :] for layer in last_4_layers]\n",
    "        return torch.cat(cls_embeddings, dim=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling_strategy == 'mean':\n",
    "            feature = self._masked_mean_pooling(last_hidden_state, attention_mask)\n",
    "            output = self.fc(feature)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1':\n",
    "            glob, last_tok, q, a = self._get_pooling_features(last_hidden_state, attention_mask, token_type_ids)\n",
    "            q_feat = torch.cat([glob, last_tok, q], dim=1)\n",
    "            a_feat = torch.cat([glob, last_tok, a], dim=1)\n",
    "            output = torch.cat([self.q_head(q_feat), self.a_head(a_feat)], dim=1)\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch1_6groups':\n",
    "            glob, last_tok, q, a = self._get_pooling_features(last_hidden_state, attention_mask, token_type_ids)\n",
    "            # Llama: use shared feature (mean + context)\n",
    "            feat_shared = torch.cat([glob, last_tok, q], dim=1)\n",
    "            \n",
    "            out_g1 = self.head_g1(feat_shared)\n",
    "            out_g2 = self.head_g2(feat_shared)\n",
    "            out_g3 = self.head_g3(feat_shared)\n",
    "            out_g4 = self.head_g4(feat_shared)\n",
    "            out_g5 = self.head_g5(feat_shared)\n",
    "            out_g6 = self.head_g6(feat_shared)\n",
    "            \n",
    "            batch_size = input_ids.size(0)\n",
    "            output = torch.zeros(batch_size, 30, dtype=out_g1.dtype, device=input_ids.device)\n",
    "            output[:, self.idx_g1] = out_g1\n",
    "            output[:, self.idx_g2] = out_g2\n",
    "            output[:, self.idx_g3] = out_g3\n",
    "            output[:, self.idx_g4] = out_g4\n",
    "            output[:, self.idx_g5] = out_g5\n",
    "            output[:, self.idx_g6] = out_g6\n",
    "            \n",
    "        elif self.pooling_strategy == 'arch2':\n",
    "            feature = self._pool_arch2(outputs.hidden_states)\n",
    "            output = self.fc(feature)\n",
    "            \n",
    "        return output\n",
    "\n",
    "# ------------------- Post-Processing Strategies -------------------\n",
    "\n",
    "# 1. Optimized Rounder (閾值截斷)\n",
    "class OptimizedRounder:\n",
    "    def __init__(self):\n",
    "        self.coef_ = [0.05, 0.95]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.nan_to_num(X, nan=0.5)\n",
    "        X_p = np.copy(X)\n",
    "        low, high = self.coef_[0], self.coef_[1]\n",
    "        X_p = np.clip(X_p, low, high)\n",
    "        if np.unique(X_p).size == 1:\n",
    "            eps = 1e-6\n",
    "            max_idx = np.argmax(X)\n",
    "            X_p[max_idx] += eps\n",
    "        return X_p\n",
    "\n",
    "# 2. Voters Rounder (網格吸附 - 防呆修正版)\n",
    "class VotersRounder:\n",
    "    def __init__(self, train_vals, dev_threshold=0.01):\n",
    "        \"\"\"\n",
    "        Deviation-aware voters rounder\n",
    "        - Builds a clean grid from training values\n",
    "        - Snaps predictions to nearest grid value\n",
    "        - If snapped values collapse (std < threshold), fallback to original predictions\n",
    "        \"\"\"\n",
    "        clean_vals = train_vals[~np.isnan(train_vals)]\n",
    "        self.unique_vals = np.sort(np.unique(clean_vals))\n",
    "        self.dev_threshold = dev_threshold\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_clean = np.nan_to_num(X, nan=0.5)\n",
    "        idx = np.abs(X_clean[:, None] - self.unique_vals[None, :]).argmin(axis=1)\n",
    "        X_p = self.unique_vals[idx]\n",
    "        \n",
    "        deviation = np.std(X_p)\n",
    "        if deviation < self.dev_threshold:\n",
    "            return X_clean\n",
    "        return X_p\n",
    "\n",
    "# 3. Distribution Rounder (分佈擬合)\n",
    "class DistributionRounder:\n",
    "    def __init__(self, train_vals):\n",
    "        self.train_vals = np.sort(train_vals)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        n = len(X)\n",
    "        ranks = rankdata(X, method='ordinal') - 1\n",
    "        return np.interp(\n",
    "            np.linspace(0, 1, n),\n",
    "            np.linspace(0, 1, len(self.train_vals)),\n",
    "            self.train_vals\n",
    "        )[ranks]\n",
    "\n",
    "# ------------------- Inference Logic -------------------\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch.get('token_type_ids')\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            y_preds = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds.append(y_preds.sigmoid().cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "if __name__ == '__main__':\n",
    "    TEST_PATH = 'test.csv'\n",
    "    TRAIN_PATH = 'train.csv'\n",
    "    \n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        TEST_PATH = '/kaggle/input/google-quest-challenge/test.csv'\n",
    "        TRAIN_PATH = '/kaggle/input/google-quest-challenge/train.csv'\n",
    "    \n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    train = pd.read_csv(TRAIN_PATH)\n",
    "    \n",
    "    print(f\"Base Model: {CFG.base_model}\")\n",
    "    print(f\"Test Shape: {test.shape}, Train Shape: {train.shape}\")\n",
    "    print(f\"Selected Post-Processing Strategy: {CFG.post_processing}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.base_model, trust_remote_code=CFG.trust_remote_code)\n",
    "    # Llama tokenizer needs pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    test_dataset = QuestDataset(test, tokenizer, max_len=CFG.max_len)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CFG.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 掃描權重：保留原有的命名規則 (與訓練腳本一致)\n",
    "    weight_paths = []\n",
    "    for fold in range(CFG.n_fold):\n",
    "        path = os.path.join(CFG.model_dir, f\"deberta_v3_fold{fold}_best.pth\")\n",
    "        if os.path.exists(path): weight_paths.append(path)\n",
    "    single_run = os.path.join(CFG.model_dir, \"deberta_v3_single_run_best.pth\")\n",
    "    if os.path.exists(single_run):\n",
    "        weight_paths.append(single_run)\n",
    "        \n",
    "    if not weight_paths:\n",
    "        print(\"No weights found in model_dir. Please set CFG.model_dir to your Llama checkpoints.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"Found {len(weight_paths)} candidate checkpoints. Building models and filtering incompatible ones...\")\n",
    "    \n",
    "    model_preds = []\n",
    "    used_weights = 0\n",
    "    for weight_path in weight_paths:\n",
    "        model = QuestModel(\n",
    "            CFG.base_model, \n",
    "            num_targets=len(TARGET_COLS), \n",
    "            pooling_strategy=CFG.pooling_strategy,\n",
    "            freeze_backbone=CFG.freeze_backbone,\n",
    "            trust_remote_code=CFG.trust_remote_code\n",
    "        )\n",
    "        model.to(CFG.device)\n",
    "\n",
    "        try:\n",
    "            state = torch.load(weight_path, map_location='cpu')\n",
    "            model.load_state_dict(state, strict=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[Skip] Incompatible checkpoint: {os.path.basename(weight_path)} -> {e}\")\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "        \n",
    "        preds = inference_fn(test_loader, model, CFG.device)\n",
    "        model_preds.append(preds)\n",
    "        used_weights += 1\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    if used_weights == 0:\n",
    "        print(\"All checkpoints were incompatible with the selected base model.\\n\"\n",
    "              \"- Ensure CFG.base_model matches the model used during training (e.g., meta-llama/Llama-3.2-1B).\\n\"\n",
    "              \"- Ensure CFG.model_dir points to your Llama-trained weights, not DeBERTa/Qwen weights.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if len(model_preds) > 0:\n",
    "        avg_preds = np.mean(model_preds, axis=0)\n",
    "        final_preds = np.zeros_like(avg_preds)\n",
    "        \n",
    "        print(f\"Applying Post-Processing: {CFG.post_processing}\")\n",
    "        \n",
    "        for i, col in enumerate(TARGET_COLS):\n",
    "            train_col_values = train[col].values\n",
    "            curr_preds = avg_preds[:, i]\n",
    "            \n",
    "            if CFG.post_processing == 'raw':\n",
    "                final_preds[:, i] = curr_preds\n",
    "                \n",
    "            elif CFG.post_processing == 'optimized':\n",
    "                opt = OptimizedRounder()\n",
    "                final_preds[:, i] = opt.predict(curr_preds)\n",
    "                \n",
    "            elif CFG.post_processing == 'voters':\n",
    "                voter = VotersRounder(train_col_values)\n",
    "                final_preds[:, i] = voter.predict(curr_preds)\n",
    "                \n",
    "            elif CFG.post_processing == 'distribution':\n",
    "                dist_rounder = DistributionRounder(train_col_values)\n",
    "                final_preds[:, i] = dist_rounder.predict(curr_preds)\n",
    "        \n",
    "        # 儲存提交檔\n",
    "        try:\n",
    "            sub_path = '/kaggle/input/google-quest-challenge/sample_submission.csv'\n",
    "            submission = pd.read_csv(sub_path)\n",
    "        except Exception:\n",
    "            submission = pd.DataFrame({'qa_id': test['qa_id']})\n",
    "        submission[TARGET_COLS] = final_preds\n",
    "        submission.to_csv('submission.csv', index=False)\n",
    "        print(\"submission.csv saved successfully!\")\n",
    "    else:\n",
    "        print(\"Error: No predictions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 828965,
     "sourceId": 7968,
     "sourceType": "competition"
    },
    {
     "datasetId": 8889245,
     "sourceId": 13946897,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 521159,
     "modelInstanceId": 506367,
     "sourceId": 668898,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
