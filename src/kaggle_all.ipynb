{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a04b20a",
   "metadata": {},
   "source": [
    "# TPE-Optimized Ensemble Submission\n",
    "\n",
    "This notebook generates final predictions using **Tree-structured Parzen Estimator (TPE)** to find optimal weights for:\n",
    "1. **Per-model fold blending**: Combine each model's 5 folds optimally\n",
    "2. **Cross-model ensemble**: Blend predictions from 6 different transformer models\n",
    "\n",
    "Models included:\n",
    "- DeBERTa-v3-base\n",
    "- ModernBERT\n",
    "- ELECTRA\n",
    "- XLNet\n",
    "- Llama-3.2-1B\n",
    "- Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import html\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, DebertaV2TokenizerFast\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b40da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "TARGET_COLS = [\n",
    "    \"question_asker_intent_understanding\", \"question_body_critical\", \"question_conversational\",\n",
    "    \"question_expect_short_answer\", \"question_fact_seeking\", \"question_has_commonly_accepted_answer\",\n",
    "    \"question_interestingness_others\", \"question_interestingness_self\", \"question_multi_intent\",\n",
    "    \"question_not_really_a_question\", \"question_opinion_seeking\", \"question_type_choice\",\n",
    "    \"question_type_compare\", \"question_type_consequence\", \"question_type_definition\",\n",
    "    \"question_type_entity\", \"question_type_instructions\", \"question_type_procedure\",\n",
    "    \"question_type_reason_explanation\", \"question_type_spelling\", \"question_well_written\",\n",
    "    \"answer_helpful\", \"answer_level_of_information\", \"answer_plausible\", \"answer_relevance\",\n",
    "    \"answer_satisfaction\", \"answer_type_instructions\", \"answer_type_procedure\",\n",
    "    \"answer_type_reason_explanation\", \"answer_well_written\",\n",
    "]\n",
    "\n",
    "print(f\"Total targets: {len(TARGET_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af72f2",
   "metadata": {},
   "source": [
    "## Configuration & Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b784473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit paths here for local/Kaggle environments\n",
    "# ============================================================================\n",
    "\n",
    "# Auto-detect environment (local vs Kaggle)\n",
    "TEST_PATH = 'test.csv'\n",
    "TRAIN_PATH = 'train.csv'\n",
    "MODEL_WEIGHTS_DIR = '/kaggle/input/deberta-finetuned/pytorch/arch1/23'  # Main directory containing all .pth files\n",
    "PRE_CALCULATED_WEIGHTS_PATH = None  # Path to pre-calculated tpe_weights.json (if available)\n",
    "\n",
    "if not os.path.exists(TEST_PATH):\n",
    "    # Running on Kaggle\n",
    "    TEST_PATH = '/kaggle/input/google-quest-challenge/test.csv'\n",
    "    TRAIN_PATH = '/kaggle/input/google-quest-challenge/train.csv'\n",
    "    MODEL_WEIGHTS_DIR = '/kaggle/input/deberta-finetuned/pytorch/arch1/23'  # Update this path on Kaggle\n",
    "\n",
    "print(f\"Environment detected:\")\n",
    "print(f\"  Train: {TRAIN_PATH}\")\n",
    "print(f\"  Test: {TEST_PATH}\")\n",
    "print(f\"  Model weights: {MODEL_WEIGHTS_DIR}\")\n",
    "if PRE_CALCULATED_WEIGHTS_PATH and os.path.exists(PRE_CALCULATED_WEIGHTS_PATH):\n",
    "    print(f\"  Pre-calculated weights: {PRE_CALCULATED_WEIGHTS_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model & Tokenizer Paths (for Kaggle, use /kaggle/input/... paths)\n",
    "# ============================================================================\n",
    "\n",
    "# Set custom paths for models/tokenizers (use local HF cache or Kaggle dataset paths)\n",
    "MODEL_PATHS = {\n",
    "    'deberta_v3': {\n",
    "        'model': '/kaggle/input/deberta-tokenizer',  # or '/kaggle/input/deberta-v3-base/...'\n",
    "        'tokenizer': '/kaggle/input/deberta-tokenizer',  # or use 'model' key if same\n",
    "    },\n",
    "    'modernbert': {\n",
    "        'model': '/kaggle/input/modernbert/transformers/base/2',\n",
    "        'tokenizer': '/kaggle/input/modernbert/transformers/base/2',  # None = use model path\n",
    "    },\n",
    "    'electra': {\n",
    "        'model': '/kaggle/input/electra',\n",
    "        'tokenizer': '/kaggle/input/electra',\n",
    "    },\n",
    "    'xlnet': {\n",
    "        'model': '/kaggle/input/xlnet0',\n",
    "        'tokenizer': '/kaggle/input/xlnet0',\n",
    "    },\n",
    "    'llama': {\n",
    "        'model': '/kaggle/input/llama-3.2/transformers/1b/1',  # or '/kaggle/input/llama-3-2-1b/...'\n",
    "        'tokenizer': '/kaggle/input/llama-3.2/transformers/1b/1',\n",
    "    },\n",
    "    'qwen': {\n",
    "        'model': '/kaggle/input/qwen-3/transformers/0.6b/1',  # or '/kaggle/input/qwen-3/transformers/0.6b/1'\n",
    "        'tokenizer': '/kaggle/input/qwen-3/transformers/0.6b/1',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Model weight filename patterns (will be searched in MODEL_WEIGHTS_DIR)\n",
    "WEIGHT_PATTERNS = {\n",
    "    'deberta_v3': 'deberta_v3_',\n",
    "    'modernbert': 'modernbert_',\n",
    "    'electra': 'electra_',\n",
    "    'xlnet': 'xlnet_',\n",
    "    'llama': 'llama_',\n",
    "    'qwen': 'qwen_',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    \"\"\"Specification for each model in the ensemble.\"\"\"\n",
    "    name: str\n",
    "    model_dir: str\n",
    "    base_model: str\n",
    "    pooling_strategy: str = \"arch1_6groups\"\n",
    "    tokenizer_path: Optional[str] = None\n",
    "    max_len: int = 512\n",
    "    batch_size: int = 8\n",
    "    pass_token_type_ids: bool = True\n",
    "    trust_remote_code: bool = False\n",
    "    weight_prefix: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Global configuration.\"\"\"\n",
    "    train_path: str = TRAIN_PATH\n",
    "    test_path: str = TEST_PATH\n",
    "    model_base_dir: str = MODEL_WEIGHTS_DIR\n",
    "    sample_frac: float = 1.0  # Use full training data for weight optimization\n",
    "    num_workers: int = 4\n",
    "    tpe_trials: int = 50  # TPE iterations for global model weights (only if no pre-calculated weights)\n",
    "    seed: int = 42\n",
    "    optimize_fold_weights: bool = False  # Set True to also optimize per-fold weights\n",
    "    use_voter_postprocessing: bool = False  # Apply VotersRounder to final preds\n",
    "    voter_dev_threshold: float = 0.01  # Std deviation guardrail for voter snapping\n",
    "    pre_calculated_weights_path: Optional[str] = PRE_CALCULATED_WEIGHTS_PATH  # Path to tpe_weights.json\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"\\nConfig loaded:\")\n",
    "print(f\"  sample_frac: {cfg.sample_frac}\")\n",
    "print(f\"  tpe_trials: {cfg.tpe_trials}\")\n",
    "print(f\"  optimize_fold_weights: {cfg.optimize_fold_weights}\")\n",
    "print(f\"  use_voter_postprocessing: {cfg.use_voter_postprocessing}\")\n",
    "print(f\"  voter_dev_threshold: {cfg.voter_dev_threshold}\")\n",
    "if cfg.pre_calculated_weights_path:\n",
    "    print(f\"  pre_calculated_weights_path: {cfg.pre_calculated_weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788587b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model specs from configuration\n",
    "MODEL_SPECS = [\n",
    "    ModelSpec(\n",
    "        name=\"deberta_v3\",\n",
    "        model_dir=cfg.model_base_dir,\n",
    "        base_model=MODEL_PATHS['deberta_v3']['model'],\n",
    "        pooling_strategy=\"arch1_6groups\",\n",
    "        tokenizer_path=MODEL_PATHS['deberta_v3']['tokenizer'],\n",
    "        batch_size=8,\n",
    "        pass_token_type_ids=True,\n",
    "        weight_prefix=WEIGHT_PATTERNS['deberta_v3'],\n",
    "    ),\n",
    "    ModelSpec(\n",
    "        name=\"modernbert\",\n",
    "        model_dir=cfg.model_base_dir,\n",
    "        base_model=MODEL_PATHS['modernbert']['model'],\n",
    "        pooling_strategy=\"arch1_6groups\",\n",
    "        tokenizer_path=MODEL_PATHS['modernbert']['tokenizer'],\n",
    "        batch_size=8,\n",
    "        pass_token_type_ids=False,\n",
    "        weight_prefix=WEIGHT_PATTERNS['modernbert'],\n",
    "    ),\n",
    "    ModelSpec(\n",
    "        name=\"electra\",\n",
    "        model_dir=cfg.model_base_dir,\n",
    "        base_model=MODEL_PATHS['electra']['model'],\n",
    "        pooling_strategy=\"arch1_6groups\",\n",
    "        tokenizer_path=MODEL_PATHS['electra']['tokenizer'],\n",
    "        batch_size=8,\n",
    "        pass_token_type_ids=True,\n",
    "        weight_prefix=WEIGHT_PATTERNS['electra'],\n",
    "    ),\n",
    "    ModelSpec(\n",
    "        name=\"xlnet\",\n",
    "        model_dir=cfg.model_base_dir,\n",
    "        base_model=MODEL_PATHS['xlnet']['model'],\n",
    "        pooling_strategy=\"arch1_6groups\",\n",
    "        tokenizer_path=MODEL_PATHS['xlnet']['tokenizer'],\n",
    "        batch_size=8,\n",
    "        pass_token_type_ids=True,\n",
    "        weight_prefix=WEIGHT_PATTERNS['xlnet'],\n",
    "    ),\n",
    "    ModelSpec(\n",
    "        name=\"llama\",\n",
    "        model_dir=cfg.model_base_dir,\n",
    "        base_model=MODEL_PATHS['llama']['model'],\n",
    "        pooling_strategy=\"arch1_6groups\",\n",
    "        tokenizer_path=MODEL_PATHS['llama']['tokenizer'],\n",
    "        batch_size=4,\n",
    "        pass_token_type_ids=False,\n",
    "        weight_prefix=WEIGHT_PATTERNS['llama'],\n",
    "    ),\n",
    "    ModelSpec(\n",
    "        name=\"qwen\",\n",
    "        model_dir=cfg.model_base_dir,\n",
    "        base_model=MODEL_PATHS['qwen']['model'],\n",
    "        pooling_strategy=\"arch1_6groups\",\n",
    "        tokenizer_path=MODEL_PATHS['qwen']['tokenizer'],\n",
    "        batch_size=4,\n",
    "        pass_token_type_ids=False,\n",
    "        weight_prefix=WEIGHT_PATTERNS['qwen'],\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"\\nConfigured {len(MODEL_SPECS)} models:\")\n",
    "for spec in MODEL_SPECS:\n",
    "    tok_path = spec.tokenizer_path or spec.base_model\n",
    "    print(f\"  â€¢ {spec.name:12s}: {spec.base_model}\")\n",
    "    print(f\"    Tokenizer: {tok_path}\")\n",
    "    print(f\"    Weights: {spec.weight_prefix}*.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1b596",
   "metadata": {},
   "source": [
    "## Data Processing & Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modern_preprocess(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "class QuestDataset(Dataset):\n",
    "    \"\"\"Dataset for Q&A pairs.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.questions = [\n",
    "            modern_preprocess(t) + \" \" + modern_preprocess(b)\n",
    "            for t, b in zip(df[\"question_title\"].values, df[\"question_body\"].values)\n",
    "        ]\n",
    "        self.answers = [modern_preprocess(a) for a in df[\"answer\"].values]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            question, answer,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        item = {\n",
    "            \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "        }\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            item[\"token_type_ids\"] = torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestModel(nn.Module):\n",
    "    \"\"\"6-head QA model with flexible pooling strategies.\"\"\"\n",
    "    def __init__(self, model_name: str, num_targets: int, pooling_strategy: str = \"arch1_6groups\",\n",
    "                 dropout_rate: float = 0.1, trust_remote_code: bool = False):\n",
    "        super().__init__()\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.config = AutoConfig.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "        if pooling_strategy == \"cls_all\":\n",
    "            self.config.update({\"output_hidden_states\": True})\n",
    "        self.backbone = AutoModel.from_pretrained(model_name, config=self.config, trust_remote_code=trust_remote_code)\n",
    "        hidden_size = self.config.hidden_size\n",
    "\n",
    "        # 6-group task indices\n",
    "        self.idx_g1 = [3, 4, 5, 16, 17]\n",
    "        self.idx_g2 = [0, 1, 6, 7, 20]\n",
    "        self.idx_g3 = [2, 10]\n",
    "        self.idx_g4 = [8, 9, 11, 12, 13, 14, 15, 18, 19]\n",
    "        self.idx_g5 = [26, 27]\n",
    "        self.idx_g6 = [21, 22, 23, 24, 25, 28, 29]\n",
    "\n",
    "        if self.pooling_strategy == \"arch1_6groups\":\n",
    "            self.head_g1 = self._make_head(hidden_size * 3, len(self.idx_g1), dropout_rate)\n",
    "            self.head_g2 = self._make_head(hidden_size * 3, len(self.idx_g2), dropout_rate)\n",
    "            self.head_g3 = self._make_head(hidden_size * 3, len(self.idx_g3), dropout_rate)\n",
    "            self.head_g4 = self._make_head(hidden_size * 3, len(self.idx_g4), dropout_rate)\n",
    "            self.head_g5 = self._make_head(hidden_size * 3, len(self.idx_g5), dropout_rate)\n",
    "            self.head_g6 = self._make_head(hidden_size * 3, len(self.idx_g6), dropout_rate)\n",
    "\n",
    "    def _make_head(self, input_dim: int, output_dim: int, dropout_rate: float):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, self.config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.config.hidden_size, output_dim),\n",
    "        )\n",
    "\n",
    "    def _masked_mean_pooling(self, hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n",
    "        return torch.sum(hidden_state * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    def _get_pooling_features(self, last_hidden_state, attention_mask, token_type_ids):\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        global_avg = self._masked_mean_pooling(last_hidden_state, attention_mask)\n",
    "        if token_type_ids is None:\n",
    "            return cls_token, global_avg, global_avg, global_avg\n",
    "        q_mask = attention_mask * (1 - token_type_ids)\n",
    "        a_mask = attention_mask * token_type_ids\n",
    "        q_avg = self._masked_mean_pooling(last_hidden_state, q_mask)\n",
    "        a_avg = self._masked_mean_pooling(last_hidden_state, a_mask)\n",
    "        return cls_token, global_avg, q_avg, a_avg\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        # Only pass token_type_ids when the backbone supports/needs it.\n",
    "        backbone_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        if token_type_ids is not None:\n",
    "            backbone_kwargs[\"token_type_ids\"] = token_type_ids\n",
    "        outputs = self.backbone(**backbone_kwargs)\n",
    "\n",
    "        cls_token, global_avg, q_avg, a_avg = self._get_pooling_features(\n",
    "            outputs.last_hidden_state, attention_mask, token_type_ids\n",
    "        )\n",
    "        feat_q = torch.cat([cls_token, global_avg, q_avg], dim=1)\n",
    "        feat_a = torch.cat([cls_token, global_avg, a_avg], dim=1)\n",
    "        \n",
    "        out_g1 = self.head_g1(feat_q)\n",
    "        out_g2 = self.head_g2(feat_q)\n",
    "        out_g3 = self.head_g3(feat_q)\n",
    "        out_g4 = self.head_g4(feat_q)\n",
    "        out_g5 = self.head_g5(feat_a)\n",
    "        out_g6 = self.head_g6(feat_a)\n",
    "\n",
    "        output = torch.zeros(input_ids.size(0), 30, dtype=out_g1.dtype, device=input_ids.device)\n",
    "        output[:, self.idx_g1] = out_g1\n",
    "        output[:, self.idx_g2] = out_g2\n",
    "        output[:, self.idx_g3] = out_g3\n",
    "        output[:, self.idx_g4] = out_g4\n",
    "        output[:, self.idx_g5] = out_g5\n",
    "        output[:, self.idx_g6] = out_g6\n",
    "        return output\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacbd2c",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac34d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_spearman(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Calculate mean Spearman correlation across all targets.\"\"\"\n",
    "    scores = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        score, _ = spearmanr(y_true[:, i], y_pred[:, i])\n",
    "        scores.append(0.0 if np.isnan(score) else score)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "def get_weight_paths(model_dir: str, prefix: Optional[str]) -> List[str]:\n",
    "    \"\"\"Get sorted list of .pth files matching the prefix.\"\"\"\n",
    "    files = [f for f in os.listdir(model_dir) if f.endswith(\".pth\")]\n",
    "    if prefix:\n",
    "        files = [f for f in files if f.startswith(prefix)]\n",
    "    paths = [os.path.join(model_dir, f) for f in files]\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No .pth files found in {model_dir} with prefix '{prefix or '*'}'\")\n",
    "    return sorted(paths)\n",
    "\n",
    "\n",
    "def build_loader(df: pd.DataFrame, spec: ModelSpec) -> Tuple[DataLoader, any]:\n",
    "    \"\"\"Build DataLoader for a given model spec.\"\"\"\n",
    "    tokenizer_name = spec.tokenizer_path or spec.base_model\n",
    "\n",
    "    # Explicitly use the fast DeBERTa tokenizer so the local SentencePiece vocab is picked up.\n",
    "    if spec.name.startswith(\"deberta\"):\n",
    "        tokenizer = DebertaV2TokenizerFast.from_pretrained(\n",
    "            tokenizer_name,\n",
    "            trust_remote_code=spec.trust_remote_code,\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_name,\n",
    "            trust_remote_code=spec.trust_remote_code,\n",
    "        )\n",
    "    \n",
    "    # Ensure tokenizer has a padding token (required for models like Llama, Qwen)\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    dataset = QuestDataset(df, tokenizer, max_len=spec.max_len)\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=spec.batch_size, shuffle=False,\n",
    "        num_workers=cfg.num_workers, pin_memory=True\n",
    "    )\n",
    "    return loader, tokenizer\n",
    "\n",
    "\n",
    "class VotersRounder:\n",
    "    \"\"\"Snap predictions to nearest observed training values with a deviation guard.\"\"\"\n",
    "\n",
    "    def __init__(self, train_vals: np.ndarray, dev_threshold: float = 0.01):\n",
    "        clean_vals = train_vals[~np.isnan(train_vals)]\n",
    "        self.unique_vals = np.sort(np.unique(clean_vals))\n",
    "        self.dev_threshold = dev_threshold\n",
    "\n",
    "    def predict(self, preds: np.ndarray) -> np.ndarray:\n",
    "        preds = np.nan_to_num(preds, nan=0.5)\n",
    "        idx = np.abs(preds[:, None] - self.unique_vals[None, :]).argmin(axis=1)\n",
    "        snapped = self.unique_vals[idx]\n",
    "        # If snapping collapses variance, fall back to raw predictions\n",
    "        if np.std(snapped) < self.dev_threshold:\n",
    "            return preds\n",
    "        return snapped\n",
    "\n",
    "\n",
    "def apply_voter_postprocessing(preds: np.ndarray, train_df: pd.DataFrame, dev_threshold: float) -> np.ndarray:\n",
    "    \"\"\"Apply VotersRounder per target using training column distributions.\"\"\"\n",
    "    rounded = preds.copy()\n",
    "    for i, col in enumerate(TARGET_COLS):\n",
    "        voter = VotersRounder(train_df[col].values, dev_threshold=dev_threshold)\n",
    "        rounded[:, i] = voter.predict(preds[:, i])\n",
    "    return rounded\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e435499",
   "metadata": {},
   "source": [
    "## Inference & Ensemble Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_single_model(loader: DataLoader, weight_path: str, spec: ModelSpec) -> np.ndarray:\n",
    "    \"\"\"Run inference with a single model checkpoint.\"\"\"\n",
    "    model = QuestModel(\n",
    "        model_name=spec.base_model,\n",
    "        num_targets=len(TARGET_COLS),\n",
    "        pooling_strategy=spec.pooling_strategy,\n",
    "        trust_remote_code=spec.trust_remote_code,\n",
    "    )\n",
    "    state = torch.load(weight_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=f\"{spec.name}:{os.path.basename(weight_path)}\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch.get(\"token_type_ids\")\n",
    "            if token_type_ids is not None and spec.pass_token_type_ids:\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "            else:\n",
    "                token_type_ids = None\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds.append(outputs.sigmoid().cpu().numpy())\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "\n",
    "def collect_fold_predictions(df: pd.DataFrame, test_df: pd.DataFrame, spec: ModelSpec, skip_train: bool = False) -> Tuple[Optional[np.ndarray], np.ndarray]:\n",
    "    \"\"\"Collect predictions from all available folds for a single model.\n",
    "\n",
    "    If skip_train is True, only test predictions are generated (no train forward pass).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {spec.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build loaders\n",
    "    loader = None\n",
    "    if not skip_train:\n",
    "        loader, _ = build_loader(df, spec)\n",
    "    test_loader, _ = build_loader(test_df, spec)\n",
    "    fold_paths = get_weight_paths(spec.model_dir, spec.weight_prefix)\n",
    "    \n",
    "    print(f\"Found {len(fold_paths)} fold checkpoint(s)\")\n",
    "    \n",
    "    train_fold_preds = [] if not skip_train else None\n",
    "    test_fold_preds = []\n",
    "    \n",
    "    for path in fold_paths:\n",
    "        print(f\"  Loading {os.path.basename(path)}...\")\n",
    "        if not skip_train:\n",
    "            train_fold_preds.append(inference_single_model(loader, path, spec))\n",
    "        test_fold_preds.append(inference_single_model(test_loader, path, spec))\n",
    "    \n",
    "    train_stack = np.stack(train_fold_preds) if train_fold_preds is not None else None\n",
    "    test_stack = np.stack(test_fold_preds)\n",
    "    return train_stack, test_stack\n",
    "\n",
    "print(\"Inference functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ef177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpe_weight_search(preds: np.ndarray, y_true: np.ndarray, max_evals: int, label: str = \"model\") -> np.ndarray:\n",
    "    \"\"\"Use TPE to find optimal weights for blending predictions.\"\"\"\n",
    "    n = preds.shape[0]\n",
    "    if n == 1:\n",
    "        return np.ones(1, dtype=np.float32)\n",
    "\n",
    "    space = {f\"w_{i}\": hp.uniform(f\"w_{i}\", 0.0, 1.0) for i in range(n)}\n",
    "\n",
    "    def objective(params: Dict[str, float]) -> Dict[str, float]:\n",
    "        weights = np.array([params[f\"w_{i}\"] for i in range(n)], dtype=np.float64)\n",
    "        weights = np.clip(weights, 1e-6, None)\n",
    "        weights = weights / weights.sum()\n",
    "        blended = np.tensordot(weights, preds, axes=((0), (0)))\n",
    "        score = mean_spearman(y_true, blended)\n",
    "        return {\"loss\": -score, \"status\": STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=objective, space=space, algo=tpe.suggest,\n",
    "        max_evals=max_evals, trials=trials, verbose=0\n",
    "    )\n",
    "    \n",
    "    raw = np.array([best_params[f\"w_{i}\"] for i in range(n)], dtype=np.float64)\n",
    "    raw = np.clip(raw, 1e-6, None)\n",
    "    weights = raw / raw.sum()\n",
    "    \n",
    "    print(f\"\\n{label.upper()} WEIGHTS:\")\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"  {label}_{i}: {w:.4f}\")\n",
    "    \n",
    "    return weights.astype(np.float32)\n",
    "\n",
    "\n",
    "def blend_preds(preds: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Blend predictions using weights.\"\"\"\n",
    "    return np.tensordot(weights, preds, axes=((0), (0)))\n",
    "\n",
    "print(\"TPE optimization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201b400",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a01f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(cfg.train_path)\n",
    "test_df = pd.read_csv(cfg.test_path)\n",
    "\n",
    "if cfg.sample_frac < 1.0:\n",
    "    train_df = train_df.sample(frac=cfg.sample_frac, random_state=cfg.seed).reset_index(drop=True)\n",
    "    print(f\"Using {cfg.sample_frac:.1%} of training data: {len(train_df)} samples\")\n",
    "else:\n",
    "    print(f\"Using full training data: {len(train_df)} samples\")\n",
    "\n",
    "print(f\"Test data: {len(test_df)} samples\")\n",
    "\n",
    "y_true = train_df[TARGET_COLS].values\n",
    "print(f\"Target shape: {y_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ecc986",
   "metadata": {},
   "source": [
    "## Step 1: Collect All Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020efb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_preds = []\n",
    "model_test_preds = []\n",
    "fold_weights_all = {}\n",
    "\n",
    "# Try to load pre-calculated weights\n",
    "pre_calculated_weights = None\n",
    "if cfg.pre_calculated_weights_path and os.path.exists(cfg.pre_calculated_weights_path):\n",
    "    print(f\"\\nLoading pre-calculated weights from: {cfg.pre_calculated_weights_path}\")\n",
    "    with open(cfg.pre_calculated_weights_path, \"r\") as f:\n",
    "        pre_calculated_weights = json.load(f)\n",
    "    print(f\"âœ“ Loaded weights with score: {pre_calculated_weights.get('final_score', 'N/A'):.4f}\")\n",
    "\n",
    "test_only = pre_calculated_weights is not None\n",
    "if test_only:\n",
    "    print(\"Test-only mode: skipping train inference because weights are provided.\")\n",
    "\n",
    "for spec in MODEL_SPECS:\n",
    "    # Get predictions from all available folds (train optional)\n",
    "    train_folds, test_folds = collect_fold_predictions(train_df, test_df, spec, skip_train=test_only)\n",
    "    \n",
    "    # Determine fold weights\n",
    "    if pre_calculated_weights and \"fold_weights\" in pre_calculated_weights and spec.name in pre_calculated_weights[\"fold_weights\"]:\n",
    "        # Use pre-calculated fold weights\n",
    "        fold_weights = np.array(pre_calculated_weights[\"fold_weights\"][spec.name], dtype=np.float32)\n",
    "        print(f\"Using pre-calculated fold weights for {spec.name}\")\n",
    "    elif cfg.optimize_fold_weights and train_folds is not None:\n",
    "        # Use TPE to optimize fold weights\n",
    "        print(f\"Optimizing fold weights for {spec.name} using TPE...\")\n",
    "        fold_weights = tpe_weight_search(\n",
    "            train_folds, y_true,\n",
    "            max_evals=min(cfg.tpe_trials, 30),  # Fewer trials for fold optimization\n",
    "            label=f\"{spec.name}_fold\"\n",
    "        )\n",
    "    else:\n",
    "        # Use equal weights across all folds\n",
    "        fold_weights = np.ones(test_folds.shape[0], dtype=np.float32) / test_folds.shape[0]\n",
    "        print(f\"Using equal weights over {test_folds.shape[0]} fold(s)\")\n",
    "    \n",
    "    fold_weights_all[spec.name] = fold_weights.tolist()\n",
    "    \n",
    "    # Blend folds\n",
    "    blended_train = blend_preds(train_folds, fold_weights) if train_folds is not None else None\n",
    "    blended_test = blend_preds(test_folds, fold_weights)\n",
    "    \n",
    "    # Evaluate\n",
    "    if blended_train is not None:\n",
    "        score = mean_spearman(y_true, blended_train)\n",
    "        print(f\"{spec.name} blended score: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{spec.name}: train inference skipped (test-only mode)\")\n",
    "    \n",
    "    if blended_train is not None:\n",
    "        model_train_preds.append(blended_train)\n",
    "    model_test_preds.append(blended_test)\n",
    "\n",
    "model_train_preds_np = np.stack(model_train_preds) if model_train_preds else None\n",
    "model_test_preds_np = np.stack(model_test_preds)\n",
    "\n",
    "print(f\"\\nCollected predictions from {len(MODEL_SPECS)} models\")\n",
    "if model_train_preds_np is not None:\n",
    "    print(f\"Train predictions shape: {model_train_preds_np.shape}\")\n",
    "print(f\"Test predictions shape: {model_test_preds_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea92d4",
   "metadata": {},
   "source": [
    "## Step 2: TPE Optimization Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b767e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pre_calculated_weights and \"model_weights\" in pre_calculated_weights:\n",
    "    # Use pre-calculated weights\n",
    "    print(f\"\\nUsing pre-calculated model weights...\")\n",
    "    model_weights = np.array([\n",
    "        pre_calculated_weights[\"model_weights\"].get(spec.name, 1.0/len(MODEL_SPECS))\n",
    "        for spec in MODEL_SPECS\n",
    "    ], dtype=np.float32)\n",
    "    model_weights = model_weights / model_weights.sum()  # Re-normalize\n",
    "else:\n",
    "    if model_train_preds_np is None:\n",
    "        raise ValueError(\"No training predictions available to run TPE. Provide pre-calculated weights or enable train inference.\")\n",
    "    # Run TPE optimization\n",
    "    print(f\"Running TPE optimization across {len(MODEL_SPECS)} models...\")\n",
    "    print(f\"TPE trials: {cfg.tpe_trials}\")\n",
    "    model_weights = tpe_weight_search(\n",
    "        model_train_preds_np, y_true, \n",
    "        max_evals=cfg.tpe_trials, \n",
    "        label=\"model\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL WEIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "for spec, weight in zip(MODEL_SPECS, model_weights):\n",
    "    print(f\"{spec.name:20s}: {weight:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacadbf8",
   "metadata": {},
   "source": [
    "## Step 3: Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = blend_preds(model_train_preds_np, model_weights) if model_train_preds_np is not None else None\n",
    "final_test = blend_preds(model_test_preds_np, model_weights)\n",
    "\n",
    "if cfg.use_voter_postprocessing:\n",
    "    print(\"Applying VotersRounder post-processing...\")\n",
    "    if final_train is not None:\n",
    "        final_train = apply_voter_postprocessing(final_train, train_df, dev_threshold=cfg.voter_dev_threshold)\n",
    "    final_test = apply_voter_postprocessing(final_test, train_df, dev_threshold=cfg.voter_dev_threshold)\n",
    "\n",
    "if final_train is not None:\n",
    "    final_score = mean_spearman(y_true, final_train)\n",
    "else:\n",
    "    final_score = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "if final_score is not None:\n",
    "    print(f\"Training Spearman Score: {final_score:.4f}\")\n",
    "else:\n",
    "    print(\"Training Spearman Score: skipped (test-only mode)\")\n",
    "print(f\"Final test predictions shape: {final_test.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1c150",
   "metadata": {},
   "source": [
    "## Step 4: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission to get correct format\n",
    "sample_sub = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "# Create submission\n",
    "submission = sample_sub[['qa_id']].copy()\n",
    "submission[TARGET_COLS] = final_test\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"âœ“ Submission file created: submission.csv\")\n",
    "print(f\"  Shape: {submission.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111ae07",
   "metadata": {},
   "source": [
    "## Save Weights & OOF Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out-of-fold predictions (only if available)\n",
    "if final_train is not None:\n",
    "    np.save(\"tpe_oof_predictions.npy\", final_train)\n",
    "    print(\"âœ“ Saved OOF predictions: tpe_oof_predictions.npy\")\n",
    "else:\n",
    "    print(\"Skipping OOF save (test-only mode)\")\n",
    "\n",
    "# Save weights\n",
    "weight_payload = {\n",
    "    \"final_score\": None if final_score is None else float(final_score),\n",
    "    \"model_weights\": {spec.name: float(w) for spec, w in zip(MODEL_SPECS, model_weights)},\n",
    "    \"fold_weights\": fold_weights_all,\n",
    "    \"config\": {\n",
    "        \"tpe_trials\": cfg.tpe_trials,\n",
    "        \"optimize_fold_weights\": cfg.optimize_fold_weights,\n",
    "        \"sample_frac\": cfg.sample_frac,\n",
    "        \"test_only\": test_only,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"tpe_weights.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(weight_payload, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Saved weights: tpe_weights.json\")\n",
    "print(\"\\nWeight summary:\")\n",
    "print(json.dumps(weight_payload[\"model_weights\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca992d83",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**TPE-Optimized Ensemble Complete! ðŸŽ‰**\n",
    "\n",
    "This notebook:\n",
    "1. âœ… Loaded predictions from **5 transformer models** (DeBERTa, ModernBERT, ELECTRA, XLNet, Llama)\n",
    "2. âœ… Optimized fold blending for each model (mean or TPE-weighted)\n",
    "3. âœ… Found optimal model weights using **Tree-structured Parzen Estimator (TPE)**\n",
    "4. âœ… Generated final predictions on test set\n",
    "5. âœ… Created submission file: `submission.csv`\n",
    "6. âœ… Saved weights and OOF predictions for analysis\n",
    "\n",
    "The final ensemble achieves better performance than any single model by intelligently weighting each model's strengths!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 828965,
     "isSourceIdPinned": false,
     "sourceId": 7968,
     "sourceType": "competition"
    },
    {
     "datasetId": 9117211,
     "sourceId": 14284169,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9117426,
     "sourceId": 14284474,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8889245,
     "sourceId": 14290729,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 301506,
     "sourceId": 363124,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 121027,
     "modelInstanceId": 100931,
     "sourceId": 120000,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 216571,
     "modelInstanceId": 194672,
     "sourceId": 257174,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 521159,
     "modelInstanceId": 506367,
     "sourceId": 698246,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
